"""
Script: model_train_insurance.py
Description: PySpark regression training script for medical insurance cost prediction
             –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–∂–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ features.
"""

import os
import sys
import numpy as np
import traceback
import argparse
import mlflow
import mlflow.spark
from mlflow.tracking import MlflowClient

from pyspark.sql import SparkSession
from pyspark.ml.regression import RandomForestRegressor, GBTRegressor, LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder


def create_spark_session(s3_config=None):
    """
    Create and configure a Spark session.

    Parameters
    ----------
    s3_config : dict, optional
        Dictionary containing S3 configuration parameters
        (endpoint_url, access_key, secret_key)

    Returns
    -------
    SparkSession
        Configured Spark session
    """
    print("DEBUG: –ù–∞—á–∏–Ω–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ Spark —Å–µ—Å—Å–∏–∏")
    try:
        # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π Builder
        builder = (SparkSession
            .builder
            .appName("FraudDetectionModel")
        )

        # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è S3, –¥–æ–±–∞–≤–ª—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        if s3_config and all(k in s3_config for k in ['endpoint_url', 'access_key', 'secret_key']):
            print(f"DEBUG: –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º S3 —Å endpoint_url: {s3_config['endpoint_url']}")
            builder = (builder
                .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
                .config("spark.hadoop.fs.s3a.endpoint", s3_config['endpoint_url'])
                .config("spark.hadoop.fs.s3a.access.key", s3_config['access_key'])
                .config("spark.hadoop.fs.s3a.secret.key", s3_config['secret_key'])
                .config("spark.hadoop.fs.s3a.path.style.access", "true")
                .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "true")
            )

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–∞–º—è—Ç–∏ –∏ —Ä–µ—Å—É—Ä—Å–æ–≤ Spark
        builder = (builder
            .config("spark.executor.memory", "8g")
            .config("spark.driver.memory", "4g")
            .config("spark.executor.cores", "4")
            .config("spark.sql.shuffle.partitions", "200")
        )

        print("DEBUG: Spark —Å–µ—Å—Å–∏—è —É—Å–ø–µ—à–Ω–æ —Å–∫–æ–Ω—Ñ–∏–≥—É—Ä–∏—Ä–æ–≤–∞–Ω–∞")
        # –°–æ–∑–¥–∞–µ–º –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–µ—Å—Å–∏—é Spark
        return builder
    except Exception as e:
        print(f"ERROR: –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Spark —Å–µ—Å—Å–∏–∏: {str(e)}")
        print(f"Traceback: {traceback.format_exc()}")
        raise


def load_data(spark, input_path):
    """
    Load and prepare the fraud detection dataset.

    Parameters
    ----------
    spark : SparkSession
        Spark session
    input_path : str
        Path to the input data

    Returns
    -------
    tuple
        (train_df, test_df) - Spark DataFrames for training and testing
    """
    print(f"DEBUG: –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö –∏–∑: {input_path}")
    try:
        # Load the data
        print(f"DEBUG: –ß—Ç–µ–Ω–∏–µ parquet –¥–∞–Ω–Ω—ã—Ö –∏–∑ {input_path}")

        train_df = spark.read.parquet(f'{input_path}/train.parquet')
        test_df = spark.read.parquet(f'{input_path}/test.parquet')

        return train_df, test_df
    except Exception as e:
        print(f"ERROR: –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
        print(f"Traceback: {traceback.format_exc()}")
        raise

def train_model(
    train_df,
    test_df,
    label_col="charges",
    features_col="features",
    run_name="insurance_models",
    num_trees_grid=(50, 100),
    max_depth_grid=(5, 8),
    gbt_max_iter_grid=(50, 100),
    gbt_max_depth_grid=(3, 5),
    lr_reg_param_grid=(0.0, 0.1, 0.3) 
):
    """
    Train and compare RandomForest, GradientBoosting, and LinearRegression.
    Log only the best model to MLflow.
    """
    try:
        # === –≠–í–ê–õ–Æ–ê–¢–û–†–´ ===
        evaluator_rmse = RegressionEvaluator(labelCol=label_col, predictionCol="prediction", metricName="rmse")
        evaluator_r2 = RegressionEvaluator(labelCol=label_col, predictionCol="prediction", metricName="r2")

        # ==========================================================
        # === RANDOM FOREST (–ú–û–î–ï–õ–¨ ‚Ññ1) ============================
        # ==========================================================
        print("\n=== TRAINING RANDOM FOREST MODEL ===")
        rf = RandomForestRegressor(featuresCol=features_col, labelCol=label_col)
        rf_param_grid = (
            ParamGridBuilder()
            .addGrid(rf.numTrees, list(num_trees_grid))
            .addGrid(rf.maxDepth, list(max_depth_grid))
            .build()
        )
        rf_tvs = TrainValidationSplit(
            estimator=rf,
            estimatorParamMaps=rf_param_grid,
            evaluator=evaluator_rmse,
            trainRatio=0.8,
            parallelism=2
        )
        rf_model = rf_tvs.fit(train_df).bestModel
        rf_predictions = rf_model.transform(test_df)
        rf_rmse = evaluator_rmse.evaluate(rf_predictions)
        rf_r2 = evaluator_r2.evaluate(rf_predictions)
        print(f"RF -> RMSE: {rf_rmse:.4f}, R2: {rf_r2:.4f}")

        # ==========================================================
        # === GRADIENT BOOSTING (–ú–û–î–ï–õ–¨ ‚Ññ2) ========================
        # ==========================================================
        print("\n=== TRAINING GRADIENT BOOSTING MODEL ===")
        gbt = GBTRegressor(featuresCol=features_col, labelCol=label_col, seed=42)
        gbt_param_grid = (
            ParamGridBuilder()
            .addGrid(gbt.maxIter, list(gbt_max_iter_grid))
            .addGrid(gbt.maxDepth, list(gbt_max_depth_grid))
            .build()
        )
        gbt_tvs = TrainValidationSplit(
            estimator=gbt,
            estimatorParamMaps=gbt_param_grid,
            evaluator=evaluator_rmse,
            trainRatio=0.8,
            parallelism=2
        )
        gbt_model = gbt_tvs.fit(train_df).bestModel
        gbt_predictions = gbt_model.transform(test_df)
        gbt_rmse = evaluator_rmse.evaluate(gbt_predictions)
        gbt_r2 = evaluator_r2.evaluate(gbt_predictions)
        print(f"GBT -> RMSE: {gbt_rmse:.4f}, R2: {gbt_r2:.4f}")

        # ==========================================================
        # === LINEAR REGRESSION (–ú–û–î–ï–õ–¨ ‚Ññ3 - –î–û–ë–ê–í–õ–ï–ù–ê) ============
        # ==========================================================
        print("\n=== TRAINING LINEAR REGRESSION MODEL ===")
        lr = LinearRegression(featuresCol=features_col, labelCol=label_col)
        lr_param_grid = (
            ParamGridBuilder()
            .addGrid(lr.regParam, list(lr_reg_param_grid))
            .build()
        )
        lr_tvs = TrainValidationSplit(
            estimator=lr,
            estimatorParamMaps=lr_param_grid,
            evaluator=evaluator_rmse,
            trainRatio=0.8,
            parallelism=2
        )
        lr_model = lr_tvs.fit(train_df).bestModel
        lr_predictions = lr_model.transform(test_df)
        lr_rmse = evaluator_rmse.evaluate(lr_predictions)
        lr_r2 = evaluator_r2.evaluate(lr_predictions)
        print(f"LR -> RMSE: {lr_rmse:.4f}, R2: {lr_r2:.4f}")

        # ==========================================================
        # === –°–†–ê–í–ù–ï–ù–ò–ï –í–°–ï–• 3 –ú–û–î–ï–õ–ï–ô =============================
        # ==========================================================
        models = {
            "RandomForestRegressor": (rf_model, rf_rmse, rf_r2),
            "GBTRegressor": (gbt_model, gbt_rmse, gbt_r2),
            "LinearRegression": (lr_model, lr_rmse, lr_r2),
        }

        best_name, (best_model, best_rmse, best_r2) = min(models.items(), key=lambda x: x[1][1])
        print(f"\n=== BEST MODEL: {best_name} (RMSE={best_rmse:.4f}, R2={best_r2:.4f}) ===")

        # ==========================================================
        # === –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –í MLflow =================================
        # ==========================================================
        with mlflow.start_run(run_name=run_name) as run:
            run_id = run.info.run_id
            mlflow.log_param("best_model", best_name)
            mlflow.log_metric("rmse", float(best_rmse))
            mlflow.log_metric("r2", float(best_r2))
            mlflow.spark.log_model(best_model, "model")
            print(f"MLflow Run ID: {run_id} - Logged {best_name}")

            return best_model, {"run_id": run_id, "rmse": best_rmse, "r2": best_r2, "model": best_name}

    except Exception as e:
        mlflow.log_text(f"Traceback: {traceback.format_exc()}", "exception_while_training.txt")
        raise

# def train_model(
#     train_df,
#     test_df,
#     label_col="charges",
#     features_col="features",
#     run_name="insurance_models",
#     num_trees_grid=(50, 100),
#     max_depth_grid=(5, 8),
#     gbt_max_iter_grid=(50, 100),     # === –î–û–ë–ê–í–õ–ï–ù–û: —Å–µ—Ç–∫–∞ –¥–ª—è GBT ===
#     gbt_max_depth_grid=(3, 5),       # === –î–û–ë–ê–í–õ–ï–ù–û: —Å–µ—Ç–∫–∞ –¥–ª—è GBT ===
# ):
#     """
#     Train and compare RandomForestRegressor and GradientBoostingRegressor on preprocessed data.
#     Log only the best performing model to MLflow.
#     """
#     try:
#         # === –û–ë–©–ò–ï –≠–í–ê–õ–Æ–ê–¢–û–†–´ ===
#         evaluator_rmse = RegressionEvaluator(labelCol=label_col, predictionCol="prediction", metricName="rmse")
#         evaluator_r2 = RegressionEvaluator(labelCol=label_col, predictionCol="prediction", metricName="r2")

#         # ==========================================================
#         # === RANDOM FOREST MODEL TRAINING BLOCK ===================
#         # ==========================================================
#         rf = RandomForestRegressor(featuresCol=features_col, labelCol=label_col)

#         rf_param_grid = (
#             ParamGridBuilder()
#             .addGrid(rf.numTrees, list(num_trees_grid))
#             .addGrid(rf.maxDepth, list(max_depth_grid))
#             .build()
#         )

#         rf_tvs = TrainValidationSplit(
#             estimator=rf,
#             estimatorParamMaps=rf_param_grid,
#             evaluator=evaluator_rmse,
#             trainRatio=0.8,
#             parallelism=2
#         )

#         print("\n=== TRAINING RANDOM FOREST MODEL ===")
#         rf_model = rf_tvs.fit(train_df).bestModel

#         rf_predictions = rf_model.transform(test_df)
#         rf_rmse = evaluator_rmse.evaluate(rf_predictions)
#         rf_r2 = evaluator_r2.evaluate(rf_predictions)

#         print(f"RF -> RMSE: {rf_rmse:.4f}, R2: {rf_r2:.4f}")

#         # ==========================================================
#         # === GRADIENT BOOSTING MODEL TRAINING BLOCK ===============
#         # ==========================================================
#         print("\n=== TRAINING GRADIENT BOOSTING MODEL ===")
#         gbt = GBTRegressor(featuresCol=features_col, labelCol=label_col, seed=42)

#         gbt_param_grid = (
#             ParamGridBuilder()
#             .addGrid(gbt.maxIter, list(gbt_max_iter_grid))
#             .addGrid(gbt.maxDepth, list(gbt_max_depth_grid))
#             .build()
#         )

#         gbt_tvs = TrainValidationSplit(
#             estimator=gbt,
#             estimatorParamMaps=gbt_param_grid,
#             evaluator=evaluator_rmse,
#             trainRatio=0.8,
#             parallelism=2
#         )

#         gbt_model = gbt_tvs.fit(train_df).bestModel

#         gbt_predictions = gbt_model.transform(test_df)
#         gbt_rmse = evaluator_rmse.evaluate(gbt_predictions)
#         gbt_r2 = evaluator_r2.evaluate(gbt_predictions)

#         print(f"GBT -> RMSE: {gbt_rmse:.4f}, R2: {gbt_r2:.4f}")

#         # ==========================================================
#         # === –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –ò –í–´–ë–û–† –õ–£–ß–®–ï–ô ===================
#         # ==========================================================
#         if gbt_rmse < rf_rmse:
#             best_model = gbt_model
#             best_metrics = {"rmse": gbt_rmse, "r2": gbt_r2, "model": "GBTRegressor"}
#         else:
#             best_model = rf_model
#             best_metrics = {"rmse": rf_rmse, "r2": rf_r2, "model": "RandomForestRegressor"}

#         print(f"\n=== BEST MODEL: {best_metrics['model']} (RMSE={best_metrics['rmse']:.4f}) ===")

#         # ==========================================================
#         # === –õ–û–ì–ò–†–û–í–ê–ù–ò–ï –í MLflow –¢–û–õ–¨–ö–û –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò ===========
#         # ==========================================================
#         with mlflow.start_run(run_name=run_name) as run:
#             run_id = run.info.run_id

#             mlflow.log_param("best_model", best_metrics["model"])
#             mlflow.log_metric("rmse", float(best_metrics["rmse"]))
#             mlflow.log_metric("r2", float(best_metrics["r2"]))
#             mlflow.spark.log_model(best_model, "model")

#             print(f"MLflow Run ID: {run_id} - Logged {best_metrics['model']}")

#             return best_model, {"run_id": run_id, **best_metrics}

#     except Exception as e:
#         mlflow.log_text(f"Traceback: {traceback.format_exc()}", "exception_while_training.txt")
#         raise


# def train_model(
#     train_df,
#     test_df,
#     label_col="charges",
#     features_col="features",
#     run_name="insurance_rf",
#     num_trees_grid=(50, 100),
#     max_depth_grid=(5, 8),
# ):
#     """
#     Train RandomForestRegressor on preprocessed data (features vector already present).

#     Args:
#         train_df (DataFrame): training Spark DataFrame (must contain `features` and `charges` by default)
#         test_df (DataFrame): testing Spark DataFrame
#         label_col (str): name of label column (default "charges")
#         features_col (str): name of features vector column (default "features")
#         run_name (str): MLflow run name
#         num_trees_grid (iterable): values for numTrees grid
#         max_depth_grid (iterable): values for maxDepth grid

#     Returns:
#         best_model (PipelineModel / RandomForestRegressionModel): trained model
#         dict: metrics {"run_id", "rmse", "r2"}
#     """
#     try:
#         # –°–æ–∑–¥–∞—ë–º —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä, –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—è, —á—Ç–æ –ø—Ä–∏–∑–Ω–∞–∫–∏ —É–∂–µ —Å–æ–±—Ä–∞–Ω—ã –≤ features_col
#         rf = RandomForestRegressor(featuresCol=features_col, labelCol=label_col)

#         # –ü–∞—Ä–∞–º–µ—Ç—Ä–∏—á–µ—Å–∫–∞—è —Å–µ—Ç–∫–∞
#         param_grid = (
#             ParamGridBuilder()
#             .addGrid(rf.numTrees, list(num_trees_grid))
#             .addGrid(rf.maxDepth, list(max_depth_grid))
#             .build()
#         )

#         # –û—Ü–µ–Ω—â–∏–∫–∏ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
#         evaluator_rmse = RegressionEvaluator(labelCol=label_col, predictionCol="prediction", metricName="rmse")
#         evaluator_r2 = RegressionEvaluator(labelCol=label_col, predictionCol="prediction", metricName="r2")

#         # TrainValidationSplit
#         tvs = TrainValidationSplit(
#             estimator=rf,
#             estimatorParamMaps=param_grid,
#             evaluator=evaluator_rmse,
#             trainRatio=0.8,
#             parallelism=2
#         )

#         # MLflow run
#         with mlflow.start_run(run_name=run_name) as run:
#             run_id = run.info.run_id

#             # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ train_df
#             tvs_model = tvs.fit(train_df)
#             best_model = tvs_model.bestModel  # RandomForestRegressionModel

#             # –ü–æ–ª—É—á–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ RandomForest
#             # rf_model = best_model.stages[-1]  # –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç–∞–¥–∏—è Pipeline
#             # best_num_trees = rf_model.getNumTrees
#             # best_max_depth = rf_model.getMaxDepth()
#             best_num_trees = best_model.getNumTrees
#             best_max_depth = best_model.getMaxDepth()


#             # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ª—É—á—à–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
#             mlflow.log_param("best_numTrees", best_num_trees)
#             mlflow.log_param("best_maxDepth", best_max_depth)

#             # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–µ
#             predictions = best_model.transform(test_df)
#             rmse = evaluator_rmse.evaluate(predictions)
#             r2 = evaluator_r2.evaluate(predictions)

#             # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏ –º–æ–¥–µ–ª—å –≤ MLflow
#             mlflow.log_metric("rmse", float(rmse))
#             mlflow.log_metric("r2", float(r2))
#             mlflow.spark.log_model(best_model, "model")

#             print(f"MLflow Run ID: {run_id} - RMSE: {rmse:.4f}, R2: {r2:.4f}")

#             return best_model, {"run_id": run_id, "rmse": rmse, "r2": r2}

#     except Exception as e:
#         mlflow.log_text(f"Traceback: {traceback.format_exc()}", "exception_while_training.txt")
#         raise

def save_model(model, output_path):
    """
    Save the trained model to the specified path.

    Parameters
    ----------
    model : PipelineModel
        Trained model
    output_path : str
        Path to save the model
    """
    print(f"DEBUG: –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –≤: {output_path}")
    try:
        model.write().overwrite().save(output_path)
        print(f"Model saved to: {output_path}")
    except Exception as e:
        print(f"ERROR: –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {str(e)}")
        print(f"Traceback: {traceback.format_exc()}")
        raise

# --------------------------------------------------------
# MLflow utilities
# --------------------------------------------------------
def get_best_model_metrics(experiment_name):
    """
    –ü–û–õ–£–ß–ê–ï–ú –ú–ï–¢–†–ò–ö–ò –õ–£–ß–®–ï–ô –†–ï–ì–†–ï–°–°–ò–û–ù–ù–û–ô –ú–û–î–ï–õ–ò –ò–ó MLflow –° –ê–õ–ò–ê–°–û–ú 'CHAMPION'

    Parameters
    ----------
    experiment_name : str
        –ò–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ MLflow

    Returns
    -------
    dict
        –ú–µ—Ç—Ä–∏–∫–∏ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –∏–ª–∏ None, –µ—Å–ª–∏ –º–æ–¥–µ–ª–∏ –Ω–µ—Ç
    """
    print(f"DEBUG: –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ '{experiment_name}'")
    client = MlflowClient()

    try:
        print(f"DEBUG: –ò—â–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç {experiment_name}")
        experiment = client.get_experiment_by_name(experiment_name)
        if not experiment:
            print(f"–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç '{experiment_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return None
        print(f"DEBUG: –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –Ω–∞–π–¥–µ–Ω, ID: {experiment.experiment_id}")
    except Exception as e:
        print(f"ERROR: –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞: {str(e)}")
        print(f"Traceback: {traceback.format_exc()}")
        return None

    try:
        model_name = f"{experiment_name}_model"
        print(f"DEBUG: –ò—â–µ–º –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å '{model_name}'")

        try:
            registered_model = client.get_registered_model(model_name)
            print(f"–ú–æ–¥–µ–ª—å '{model_name}' –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞")
            print(f"–ú–æ–¥–µ–ª—å '{model_name}' –∏–º–µ–µ—Ç {len(registered_model.latest_versions)} –≤–µ—Ä—Å–∏–π")
        except Exception as e:
            print(f"DEBUG: –ú–æ–¥–µ–ª—å '{model_name}' –µ—â–µ –Ω–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞: {str(e)}")
            return None

        model_versions = client.get_latest_versions(model_name)
        champion_version = None

        print(f"DEBUG: –ù–∞–π–¥–µ–Ω–æ {len(model_versions)} –≤–µ—Ä—Å–∏–π –º–æ–¥–µ–ª–∏")
        for version in model_versions:
            print(f"DEBUG: –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–µ—Ä—Å–∏—é {version.version}")
            if hasattr(version, 'aliases') and "champion" in version.aliases:
                print(f"DEBUG: –ù–∞–π–¥–µ–Ω 'champion' –≤ aliases: {version.aliases}")
                champion_version = version
                break
            elif hasattr(version, 'tags') and version.tags.get('alias') == "champion":
                print(f"DEBUG: –ù–∞–π–¥–µ–Ω 'champion' –≤ —Ç–µ–≥–∞—Ö: {version.tags}")
                champion_version = version
                break
            else:
                print(f"DEBUG: –í–µ—Ä—Å–∏—è {version.version} –Ω–µ –∏–º–µ–µ—Ç –∞–ª–∏–∞—Å–∞ 'champion'")
                if hasattr(version, 'aliases'):
                    print(f"DEBUG: Aliases: {version.aliases}")
                if hasattr(version, 'tags'):
                    print(f"DEBUG: Tags: {version.tags}")

        if not champion_version:
            print("–ú–æ–¥–µ–ª—å —Å –∞–ª–∏–∞—Å–æ–º 'champion' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return None

        champion_run_id = champion_version.run_id
        print(f"DEBUG: Run ID –¥–ª—è 'champion': {champion_run_id}")

        # üîπ –†–ï–ì–†–ï–°–°–ò–Ø: –º–µ–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ rmse –∏ r2
        print(f"DEBUG: –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è run_id: {champion_run_id}")
        run = client.get_run(champion_run_id)
        metrics = {
            "run_id": champion_run_id,
            "rmse": run.data.metrics.get("rmse"),
            "r2": run.data.metrics.get("r2")
        }

        print(
            f"–¢–µ–∫—É—â–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å (champion): "
            f"–≤–µ—Ä—Å–∏—è {champion_version.version}, Run ID: {champion_run_id}"
        )
        print(
            f"–ú–µ—Ç—Ä–∏–∫–∏: RMSE={metrics['rmse']:.4f}, R2={metrics['r2']:.4f}"
        )

        return metrics
    except Exception as e:
        print(f"ERROR: –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏: {str(e)}")
        print(f"Traceback: {traceback.format_exc()}")
        return None


def compare_and_register_model(new_metrics, experiment_name):
    """
    –°–†–ê–í–ù–ò–í–ê–ï–ú –ù–û–í–£–Æ –†–ï–ì–†–ï–°–°–ò–û–ù–ù–£–Æ –ú–û–î–ï–õ–¨ –° –õ–£–ß–®–ï–ô –í MLflow –ò –†–ï–ì–ò–°–¢–†–ò–†–£–ï–ú, –ï–°–õ–ò –û–ù–ê –õ–£–ß–®–ï

    Parameters
    ----------
    new_metrics : dict
        –ú–µ—Ç—Ä–∏–∫–∏ –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (–¥–æ–ª–∂–Ω—ã —Å–æ–¥–µ—Ä–∂–∞—Ç—å 'rmse' –∏ 'r2')
    experiment_name : str
        –ò–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ MLflow

    Returns
    -------
    bool
        True, –µ—Å–ª–∏ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å –±—ã–ª–∞ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ –∫–∞–∫ 'champion'
    """
    print(f"DEBUG: –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏ —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ {experiment_name}")
    client = MlflowClient()

    # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    print("DEBUG: –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏")
    best_metrics = get_best_model_metrics(experiment_name)

    # –ò–º—è –º–æ–¥–µ–ª–∏
    model_name = f"{experiment_name}_model"
    print(f"DEBUG: –ò–º—è –º–æ–¥–µ–ª–∏: {model_name}")

    # –°–æ–∑–¥–∞–µ–º –∏–ª–∏ –ø–æ–ª—É—á–∞–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
    try:
        print(f"DEBUG: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å {model_name}")
        client.get_registered_model(model_name)
        print(f"–ú–æ–¥–µ–ª—å '{model_name}' —É–∂–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞")
    except Exception as e:
        print(f"DEBUG: –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å: {str(e)}")
        client.create_registered_model(model_name)
        print(f"–°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å '{model_name}'")

    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –∫–∞–∫ –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é
    run_id = new_metrics["run_id"]
    model_uri = f"runs:/{run_id}/model"
    print(f"DEBUG: –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –∏–∑ {model_uri}")
    model_details = mlflow.register_model(model_uri, model_name)
    new_version = model_details.version
    print(f"DEBUG: –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞ –Ω–æ–≤–∞—è –≤–µ—Ä—Å–∏—è: {new_version}")

    # –†–ï–®–ï–ù–ò–ï, –î–û–õ–ñ–ù–ê –õ–ò –ú–û–î–ï–õ–¨ –°–¢–ê–¢–¨ CHAMPION
    should_promote = False

    if not best_metrics:
        should_promote = True
        print("–≠–¢–û –ü–ï–†–í–ê–Ø –†–ï–ì–ò–°–¢–†–ò–†–£–ï–ú–ê–Ø –ú–û–î–ï–õ–¨, –û–ù–ê –°–¢–ê–ù–û–í–ò–¢–°–Ø 'CHAMPION'")
    else:
        # üîπ –°–†–ê–í–ù–ò–í–ê–ï–ú –ü–û RMSE (–ß–ï–ú –ú–ï–ù–¨–®–ï, –¢–ï–ú –õ–£–ß–®–ï)
        print(f"DEBUG: –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ - —Ç–µ–∫—É—â–∏–π RMSE: {best_metrics['rmse']}, –Ω–æ–≤—ã–π RMSE: {new_metrics['rmse']}")
        if new_metrics["rmse"] < best_metrics["rmse"]:
            should_promote = True
            improvement = (best_metrics["rmse"] - new_metrics["rmse"]) / best_metrics["rmse"] * 100
            print(f"–ù–æ–≤–∞—è –º–æ–¥–µ–ª—å –ª—É—á—à–µ –Ω–∞ {improvement:.2f}% –ø–æ RMSE. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ 'champion'")
        else:
            print(f"–ù–æ–≤–∞—è –º–æ–¥–µ–ª—å —Ö—É–∂–µ —Ç–µ–∫—É—â–µ–π 'champion' –ø–æ RMSE ({new_metrics['rmse']:.4f} >= {best_metrics['rmse']:.4f})")

    # –£–°–¢–ê–ù–ê–í–õ–ò–í–ê–ï–ú –ê–õ–ò–ê–° 'CHAMPION' –î–õ–Ø –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò
    if should_promote:
        try:
            print("DEBUG: –ü—ã—Ç–∞–µ–º—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∞–ª–∏–∞—Å 'champion'")
            if hasattr(client, 'set_registered_model_alias'):
                print("DEBUG: –ò—Å–ø–æ–ª—å–∑—É–µ–º set_registered_model_alias")
                client.set_registered_model_alias(model_name, "champion", new_version)
            else:
                print("DEBUG: –ò—Å–ø–æ–ª—å–∑—É–µ–º set_model_version_tag")
                client.set_model_version_tag(model_name, new_version, "alias", "champion")
        except Exception as e:
            print(f"ERROR: –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∞–ª–∏–∞—Å–∞ 'champion': {str(e)}")
            print(f"Traceback: {traceback.format_exc()}")
            print("DEBUG: –ò—Å–ø–æ–ª—å–∑—É–µ–º set_model_version_tag (–∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç)")
            client.set_model_version_tag(model_name, new_version, "alias", "champion")

        print(f"–í–µ—Ä—Å–∏—è {new_version} –º–æ–¥–µ–ª–∏ '{model_name}' —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∫–∞–∫ 'champion'")
        return True

    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –ª—É—á—à–µ, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∞–ª–∏–∞—Å 'challenger'
    try:
        print("DEBUG: –ü—ã—Ç–∞–µ–º—Å—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∞–ª–∏–∞—Å 'challenger'")
        if hasattr(client, 'set_registered_model_alias'):
            client.set_registered_model_alias(model_name, "challenger", new_version)
        else:
            client.set_model_version_tag(model_name, new_version, "alias", "challenger")
    except Exception as e:
        print(f"ERROR: –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∞–ª–∏–∞—Å–∞ 'challenger': {str(e)}")
        print(f"Traceback: {traceback.format_exc()}")
        print("DEBUG: –ò—Å–ø–æ–ª—å–∑—É–µ–º set_model_version_tag (–∑–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç)")
        client.set_model_version_tag(model_name, new_version, "alias", "challenger")

    print(f"–í–µ—Ä—Å–∏—è {new_version} –º–æ–¥–µ–ª–∏ '{model_name}' —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∫–∞–∫ 'challenger'")
    return False

def main():
    print("DEBUG: –°–∫—Ä–∏–ø—Ç –∑–∞–ø—É—â–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é")
    parser = argparse.ArgumentParser(description="Insurance Model Training")
    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument("--input", required=True, help="Input data path")
    parser.add_argument("--output", required=True, help="Output model path")
    parser.add_argument("--model-type", default="various", help="Model type (rf or lr)")

    # MLflow –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument("--tracking-uri", help="MLflow tracking URI")
    parser.add_argument("--experiment-name", default="Insurance", help="MLflow exp name")
    parser.add_argument("--auto-register", action="store_true", help="Automatically register")
    parser.add_argument("--run-name", default=None, help="Name for the MLflow run")

    # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ Git –¥–ª—è MLflow
    os.environ['GIT_PYTHON_REFRESH'] = 'quiet'

    # S3 –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    parser.add_argument("--s3-endpoint-url", help="S3 endpoint URL")
    parser.add_argument("--s3-access-key", help="S3 access key")
    parser.add_argument("--s3-secret-key", help="S3 secret key")

    args = parser.parse_args()
    print(f"DEBUG: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏: {args}")

    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º S3 –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    s3_config = None
    if args.s3_endpoint_url and args.s3_access_key and args.s3_secret_key:
        print("DEBUG: –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º S3 –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é")
        s3_config = {
            'endpoint_url': args.s3_endpoint_url,
            'access_key': args.s3_access_key,
            'secret_key': args.s3_secret_key
        }
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è MLflow
        os.environ['AWS_ACCESS_KEY_ID'] = args.s3_access_key
        os.environ['AWS_SECRET_ACCESS_KEY'] = args.s3_secret_key
        os.environ['MLFLOW_S3_ENDPOINT_URL'] = args.s3_endpoint_url
        print("DEBUG: –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è S3 —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")

    # Set MLflow tracking URI if provided
    if args.tracking_uri:
        print(f"DEBUG: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º MLflow tracking URI: {args.tracking_uri}")
        mlflow.set_tracking_uri(args.tracking_uri)

    # Create or set the experiment
    print(f"DEBUG: –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º MLflow —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: {args.experiment_name}")
    mlflow.set_experiment(args.experiment_name)

    # Create Spark session
    print("DEBUG: –°–æ–∑–¥–∞–µ–º Spark —Å–µ—Å—Å–∏—é")
    spark = create_spark_session(s3_config).getOrCreate()
    print("DEBUG: Spark —Å–µ—Å—Å–∏—è —Å–æ–∑–¥–∞–Ω–∞")

    try:
        train_df, test_df = load_data(spark, args.input)
        # Generate run name if not provided
        run_name = args.run_name or f"insurance_{args.model_type}_{os.path.basename(args.input)}"
        print(f"DEBUG: Run name: {run_name}")

        model, metrics = train_model(train_df, test_df, run_name=run_name)

        # Save the model locally
        print("DEBUG: –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å")
        save_model(model, args.output)

        # Register model if requested
        if args.auto_register:
            print("DEBUG: –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏ —Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å")
            compare_and_register_model(metrics, args.experiment_name)

        print("Training completed successfully!")

    except Exception as e:
        print(f"ERROR: –û—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è: {str(e)}")
        print(f"Traceback: {traceback.format_exc()}")
        sys.exit(1)
    finally:
        # Stop Spark session
        print("DEBUG: –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Spark —Å–µ—Å—Å–∏—é")
        spark.stop()
        print("DEBUG: –°–∫—Ä–∏–ø—Ç –∑–∞–≤–µ—Ä—à–µ–Ω")


if __name__ == "__main__":
    main()

